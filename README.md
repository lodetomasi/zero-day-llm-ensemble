# Zero-Day Vulnerability Detection Using Multi-Agent Large Language Model Ensemble

## Abstract

This repository presents a novel approach to zero-day vulnerability detection leveraging an ensemble of specialized Large Language Models (LLMs). Our methodology addresses the critical challenge of identifying vulnerabilities that have been exploited in the wild before patches were available, using only the textual descriptions from Common Vulnerabilities and Exposures (CVE) entries. The system achieves 70% classification accuracy while maintaining strict data isolation to prevent information leakage, demonstrating the potential of LLMs in cybersecurity threat assessment.

## 1. Introduction

Zero-day vulnerabilities represent one of the most significant threats in cybersecurity, as they are actively exploited before vendors can develop and distribute patches. Traditional detection methods often rely on signatures, heuristics, or post-exploitation indicators, limiting their effectiveness for proactive defense. This research explores whether Large Language Models can identify linguistic and technical patterns in vulnerability descriptions that correlate with zero-day exploitation.

### 1.1 Research Contributions

- **Novel Multi-Agent Architecture**: We introduce a specialized ensemble of five LLM agents, each analyzing vulnerabilities from distinct perspectives (forensic, pattern-based, temporal, attribution, and meta-analytical)
- **Zero Data Leakage Protocol**: Our methodology ensures complete isolation between training data sources and model predictions, preventing the common pitfall of source-based bias
- **Open-Ended Prompting Strategy**: Unlike prescriptive approaches, our system allows models to reason freely about vulnerability characteristics
- **Empirical Validation**: Comprehensive evaluation on real-world CVE data from CISA KEV and NVD databases

## 2. Methodology

### 2.1 Data Sources and Collection

We utilize two authoritative sources:
- **CISA Known Exploited Vulnerabilities (KEV)**: Confirmed zero-day vulnerabilities
- **National Vulnerability Database (NVD)**: General vulnerability repository (~95% non-zero-day)

### 2.2 Multi-Agent Architecture

Our ensemble consists of five specialized agents:

| Agent | Model | Domain Expertise |
|-------|-------|------------------|
| **ForensicAnalyst** | Mixtral-8x22B | Exploitation indicators and attack forensics |
| **PatternDetector** | Claude Opus 4 | Linguistic anomalies and technical patterns |
| **TemporalAnalyst** | Llama 3.3 70B | Timeline analysis and disclosure patterns |
| **AttributionExpert** | DeepSeek R1 | Threat actor behavior and targeting analysis |
| **MetaAnalyst** | Gemini 2.5 Pro | Cross-agent synthesis and final classification |

### 2.3 Experimental Results

On a balanced dataset of 100 CVEs (50 zero-day, 50 regular):

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Accuracy** | 70.0% | Overall classification performance |
| **Precision** | 80.0% | Low false positive rate |
| **Recall** | 45.0% | Conservative but reliable detection |
| **F1-Score** | 0.58 | Balanced performance metric |
| **Specificity** | 95.0% | Excellent regular CVE identification |

## 3. Implementation

### 3.1 System Requirements

```bash
# Python 3.8+
git clone https://github.com/lodetomasi/zero-day-llm-ensemble.git
cd zero-day-llm-ensemble
pip install -r requirements.txt
export OPENROUTER_API_KEY="your-api-key"
```

### 3.2 Execution

```bash
# Balanced evaluation (recommended for research validation)
python run_complete_test.py --zero-days 50 --regular 50 --parallel

# Large-scale evaluation
python run_complete_test.py --zero-days 100 --regular 100 --parallel
```

#### Command-Line Parameters

```bash
python run_complete_test.py [OPTIONS]

Required Arguments:
  --zero-days N        Number of zero-day CVEs to test (from CISA KEV)
  --regular N          Number of regular CVEs to test (from NVD)

Optional Arguments:
  --parallel           Enable parallel agent execution (recommended)
  --sequential         Force sequential agent execution (default if not specified)
  --output-dir PATH    Custom output directory (default: ./results)
  --no-visualizations  Disable automatic plot generation
  --verbose            Enable detailed logging output
  --cache-ttl HOURS    Cache time-to-live in hours (default: 24)
  --timeout SECONDS    API timeout per agent in seconds (default: 60)
  --seed N             Random seed for reproducibility
  --start-year YYYY    Filter CVEs from this year onwards (default: 2020)
  --end-year YYYY      Filter CVEs up to this year (default: current year)
```

#### Advanced Usage Examples

```bash
# Reproducible experiment with specific seed
python run_complete_test.py --zero-days 25 --regular 25 --parallel --seed 42

# Test only recent vulnerabilities (2023-2024)
python run_complete_test.py --zero-days 50 --regular 50 --parallel --start-year 2023

# Custom output location with verbose logging
python run_complete_test.py --zero-days 30 --regular 30 --parallel --output-dir ~/experiments/run1 --verbose

# Quick test without visualizations
python run_complete_test.py --zero-days 10 --regular 10 --parallel --no-visualizations

# Extended timeout for slow connections
python run_complete_test.py --zero-days 20 --regular 20 --parallel --timeout 120
```

### 3.3 Alternative Execution Scripts

#### Balanced Test Script
```bash
python run_balanced_test.py [OPTIONS]

# Ensures exactly 50/50 distribution
# Automatically retries if data sources have insufficient samples
# Same parameters as run_complete_test.py
```

#### Batch Evaluation
```bash
# Run multiple experiments with different configurations
for seed in 1 2 3 4 5; do
    python run_complete_test.py --zero-days 50 --regular 50 --parallel --seed $seed
done
```

### 3.4 Output Artifacts

- `results/complete_test_TIMESTAMP.json`: Raw prediction data and agent responses
- `results/analysis_plots_TIMESTAMP.png`: Comprehensive visualization suite (6 subplots)
- `results/report_TIMESTAMP.txt`: Statistical summary and performance metrics
- `logs/experiment_TIMESTAMP.log`: Detailed execution logs (if --verbose)
- `cache/cisa_kev_cache.json`: Cached CISA KEV data (24h TTL)
- `cache/nvd_cache.json`: Cached NVD data (24h TTL)

## 4. Technical Architecture

### 4.1 Data Pipeline

1. **Collection Phase**: Automated retrieval from CISA KEV and NVD APIs with 24-hour caching
2. **Preprocessing**: Standardization of CVE entries without source indicators
3. **Parallel Analysis**: Concurrent execution of agent predictions for efficiency
4. **Ensemble Integration**: Unweighted averaging of agent predictions
5. **Binary Classification**: Threshold-based decision (P > 0.5 â†’ zero-day)

### 4.2 Prompt Engineering

Our open-ended prompting strategy avoids prescriptive patterns:

```yaml
analysis_template: |
  Analyze this vulnerability:
  
  CVE ID: {cve_id}
  Vendor: {vendor}
  Product: {product}
  Description: {description}
  
  Based on your expertise, assess the likelihood this was exploited as a zero-day.
  Consider any clues in the description, the vendor/product involved, and your knowledge
  of typical zero-day patterns.
```

### 4.3 Visualization Suite

Six automated visualizations provide comprehensive performance analysis:
- Confusion Matrix with normalized values
- Performance metrics comparison (Accuracy, Precision, Recall, F1)
- Probability distribution analysis by class
- ROC curve with AUC calculation
- Temporal prediction patterns
- Confidence-calibrated accuracy assessment

### 4.4 Configuration Options

#### Environment Variables
```bash
# Required
export OPENROUTER_API_KEY="your-api-key"

# Optional
export OPENROUTER_BASE_URL="https://openrouter.ai/api/v1"  # Custom API endpoint
export LOG_LEVEL="INFO"                                     # DEBUG, INFO, WARNING, ERROR
export CACHE_DIR="./cache"                                  # Custom cache directory
export RESULTS_DIR="./results"                              # Custom results directory
```

#### Model Configuration (`config/settings.py`)
```python
# Modify LLM models per agent
MODEL_CONFIGS = {
    'ForensicAnalyst': 'mistralai/mixtral-8x22b-instruct',
    'PatternDetector': 'anthropic/claude-opus-4',
    'TemporalAnalyst': 'meta-llama/llama-3.3-70b-instruct',
    'AttributionExpert': 'deepseek/deepseek-r1',
    'MetaAnalyst': 'google/gemini-2.5-pro'
}

# API parameters
API_TIMEOUT = 60  # seconds
MAX_RETRIES = 3
RETRY_DELAY = 5   # seconds

# Data collection parameters
MIN_CVE_YEAR = 2020
MAX_SAMPLES_PER_SOURCE = 1000
CACHE_TTL_HOURS = 24
```

#### Prompt Configuration (`config/prompts.yaml`)
```yaml
# Modify agent-specific prompts
agents:
  ForensicAnalyst:
    role: "forensic security analyst"
    analysis_template: |
      # Your custom prompt here
  
  # Additional agents...

# Global prompt settings
prompt_settings:
  temperature: 0.3
  max_tokens: 1000
  include_reasoning: true
```

## 5. Key Findings

### 5.1 Performance Analysis

- **High Specificity (95%)**: Minimal false positives on regular vulnerabilities
- **Conservative Detection**: The system favors precision over recall
- **Robust to Input Variation**: Consistent performance across different CVE years and vendors

### 5.2 Agent Contribution Analysis

Preliminary analysis suggests differential agent effectiveness:
- ForensicAnalyst excels at identifying exploitation artifacts
- TemporalAnalyst captures urgency indicators effectively
- MetaAnalyst provides balanced final assessments

## 6. Limitations and Future Work

### 6.1 Current Limitations

- **Recall Trade-off**: Conservative approach misses ~55% of zero-days
- **Computational Cost**: Full ensemble requires significant API calls
- **Language Dependency**: English-only CVE descriptions

### 6.2 Future Directions

- Investigation of few-shot learning approaches
- Integration of graph-based vulnerability relationships
- Exploration of confidence calibration techniques
- Cross-lingual vulnerability analysis

## 7. Reproducibility

All code, configurations, and prompts are provided for full reproducibility. The modular architecture supports easy substitution of LLM backends and prompt strategies.

### 7.1 Programmatic API Usage

```python
from src.ensemble.multi_agent import MultiAgentSystem
from src.data.preprocessor import DataPreprocessor
from src.data.collector import DataCollector

# Initialize components
system = MultiAgentSystem(
    parallel_execution=True,
    timeout=120,
    temperature=0.3
)
preprocessor = DataPreprocessor()
collector = DataCollector()

# Collect data
zero_days = collector.get_cisa_kev_data(limit=50)
regular = collector.get_nvd_data(limit=50, exclude_kev=True)

# Analyze single CVE
cve_data = {
    'cve_id': 'CVE-2024-1234',
    'vendor': 'Microsoft',
    'product': 'Windows',
    'description': 'Remote code execution vulnerability...',
    'year': 2024
}

processed = preprocessor.preprocess_entry(cve_data)
result = system.analyze_vulnerability(processed)

# Access detailed results
prediction = result['ensemble']['prediction']
confidence = result['ensemble']['confidence']
agent_predictions = result['agent_predictions']

print(f"Zero-day probability: {prediction:.1%}")
print(f"Confidence: {confidence:.1%}")
print("\nAgent breakdown:")
for agent, pred in agent_predictions.items():
    print(f"  {agent}: {pred['prediction']:.1%}")
```

### 7.2 Custom Agent Integration

```python
from src.agents.base_agent import BaseAgent

class CustomAgent(BaseAgent):
    def __init__(self):
        super().__init__(
            name="CustomAnalyst",
            model="your-preferred-model",
            role="custom security analyst"
        )
    
    def analyze(self, vulnerability_data):
        # Your custom analysis logic
        prompt = self.build_prompt(vulnerability_data)
        response = self.query_model(prompt)
        return self.parse_response(response)

# Add to ensemble
system.add_agent(CustomAgent())
```

## 8. Citation

If you use this work in your research, please cite:

```bibtex
@software{zero_day_llm_ensemble,
  author = {De Tomasi, Lorenzo},
  title = {Zero-Day Vulnerability Detection Using Multi-Agent Large Language Model Ensemble},
  year = {2025},
  url = {https://github.com/lodetomasi/zero-day-llm-ensemble}
}
```

## 9. Contact

For questions or collaborations, please open an issue or contact lorenzo.detomasi@graduate.univaq.it.

## License

This project is released under the MIT License. See LICENSE file for details.